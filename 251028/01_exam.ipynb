{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca998674",
   "metadata": {},
   "source": [
    "필요한 함수를 다른 파일에서 가져와 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4625c",
   "metadata": {},
   "source": [
    "### 감정 분석 자연어 처리\n",
    "1. data 폴더 안의 ratings_train.txt 파일을 로드한다.\n",
    "2. 상위 500개 데이터만 추출한다.\n",
    "    - case 2) 25,000번째부터 30,000번째의 데이터를 이용해보기\n",
    "3. 리뷰 데이터와 감정 데이터로 나눠준다.\n",
    "4. 리뷰 데이터를 토큰화한다. (Komoran 이용)\n",
    "5. 토큰화한 데이터를 Word2Vec에 학습시킨다.\n",
    "    - window = 3\n",
    "    - epochs = 10\n",
    "    - min_count = 5\n",
    "    - sg = 1\n",
    "    - seed = 42\n",
    "6. 벡터화한다. (Word2Vec, 단위 벡터의 평균 이용)\n",
    "7. 분류 모델 (SVC, Logistic)\n",
    "8. Train, Test 를 이용하여 2개의 모델 중 성능이 더 높은 모델을 찾는다.\n",
    "9. 단위 벡터의 평균의 성능과 단위벡터 + 중요도 평균의 성능 차이를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ea4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9100b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Komoran\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0c95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일 로드\n",
    "try:\n",
    "    df = pd.read_csv('../data/ratings_train.txt', sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    print(\"파일 경로를 확인해주세요.'../data/ratings_train.txt'\")\n",
    "except Exception as e:\n",
    "    # ParserError 방지 등을 위해 sep='\\t'가 필수적\n",
    "    print(f\"데이터 로드 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ce4037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             document  label\n",
       "0  9976970  아 더빙.. 진짜 짜증나네요 목소리      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2318d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        150000 non-null  int64 \n",
      " 1   document  149995 non-null  object\n",
      " 2   label     150000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 500개의 데이터만 추출\n",
    "# df2 = df.head(500)\n",
    "df2 = df.loc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3319d55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    259\n",
       "0    242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2a0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'document' 컬럼 결측치 제거 및 타입 변환 (Java Exception 방지)\n",
    "df['document'] = df['document'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8d979",
   "metadata": {},
   "source": [
    "**각 데이터가 후속 처리 과정에서 요구하는 자료형에 따라 tolist(), values 사용**\n",
    "- ```X_text``` (리뷰 텍스트)\n",
    "    - 텍스트 처리\n",
    "    - 자연어 처리(NLP), 특히 토큰화(```tokenize_komoran```)와 Word2Vec 학습을 위해 사용되는 데이터\n",
    "    - Word2Vec이나 대부분의 텍스트 처리 함수는 입력 데이터로 파이썬 리스트(List) 형태를 선호.\n",
    "    </br>특히 ```Word2Vec(sentences=...)``` 매개변수는 문장들의 리스트를 요구.\n",
    "    - ```.tolist()```의 역할: Pandas Series를 순수한 Python 리스트로 변환하여, NLP 라이브러리와의 호환성을 높이고 처리 과정에서 발생할 수 있는 Pandas 인덱싱 관련 오류를 방지.\n",
    "- ```Y_label``` (감정 라벨)\n",
    "    - 수치 학습\n",
    "    - 최종적으로 분류 모델(```SVC```, ```LogisticRegression```)의 종속 변수로 사용되는 데이터\n",
    "    - Scikit-learn 모델은 종속 변수로 NumPy 배열 형태를 선호.\n",
    "    - ```.values```의 역할: Pandas Series에서 NumPy 배열만 효율적으로 추출하여 반환합니다. 이는 불필요한 Pandas 인덱스 정보를 제외하고 순수한 숫자 배열만 모델 학습에 사용되도록 하여 처리 효율성을 높입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "# Komoran 객체 생성\n",
    "try:\n",
    "    komoran = Komoran()\n",
    "except Exception as e:\n",
    "    print(\"Komoran 초기화 오류. Java 환경 또는 PyKomoran 설치를 확인하세요: \", e)\n",
    "\n",
    "def build_tokenize():\n",
    "    try:\n",
    "        # 라이브러리 로드 -> 라이브러리가 존재하면 코드들 실행 \n",
    "        from konlpy.tag import Komoran  # 'K'moran 대문자임을 주의\n",
    "        komoran = Komoran()\n",
    "        allow_pos = ['NNP', 'NNG', 'VV', 'VA', 'SL', 'MAG']\n",
    "        def tokenize(text):\n",
    "            tokens = []\n",
    "            for word, pos in komoran.pos(text):\n",
    "                if pos in allow_pos:\n",
    "                    tokens.append(word)\n",
    "            return tokens\n",
    "        # tokenize 함수를 결과로 되돌려준다.\n",
    "        # 함수 자체를 되돌려줄 때는 함수명() 이 아닌 함수명만 되돌려줘야 함을 주의\n",
    "        return tokenize\n",
    "    except Exception as e:\n",
    "        print(\"Komoran 사용 불가: \", e)\n",
    "        return lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947434d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_tokenize 함수 호출\n",
    "tokenize = build_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9238b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 데이터와 감정 데이터로 분할\n",
    "reviews = df['document'].values\n",
    "Y_label = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f86f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [ tokenize(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec을 이용하여 학습(Skip-gram 방식)\n",
    "w2v = Word2Vec(\n",
    "    sentences= X_tokens, \n",
    "    window = 5, \n",
    "    min_count= 2, \n",
    "    sg = 1, \n",
    "    epochs= 100, \n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "wv = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe837a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed_mean(tokens):\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        if word in wv.index_to_key:\n",
    "            vecs.append(wv[word])\n",
    "    result = np.mean(vecs, axis=0) if vecs else np.zeros(wv.vector_size)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d52cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 중요도 \n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    tokenizer= tokenize, \n",
    "    lowercase= False\n",
    ").fit(reviews)\n",
    "idf = dict(\n",
    "    zip(\n",
    "        # get_feature_names_out() -> Tfidf에서 사용된 단어들의 목록\n",
    "        tfidf_vec.get_feature_names_out(), \n",
    "        # idf_ : 중요도\n",
    "        tfidf_vec.idf_\n",
    "    )\n",
    ")\n",
    "# 단어 별 단위 벡터의 평균과 idf을 곱한다. \n",
    "def sent_embed_tfidf(tokens):\n",
    "    vecs = []\n",
    "    weight = []\n",
    "    for word in tokens:\n",
    "        # tokens에 각각의 단어가 Word2Vec과 TF-IDF에 존재한다면\n",
    "        if word in wv.key_to_index and word in idf:\n",
    "            # vecs -> 단위벡터와 중요도를 곱한 값을 vecs 추가\n",
    "            vecs.append(wv[word] * idf[word])\n",
    "            # weight -> 중요도 데이터를 추가 \n",
    "            weight.append(idf[word])\n",
    "    # vecs의 데이터가 존재하지 않는다면 -> tokens 안에 단어는 존재하지만 Word2Vec이나\n",
    "    # TD-IDF에 단어가 존재하지 않을때\n",
    "    if not vecs:\n",
    "        # 희소 행렬 되돌려준다. 0행렬\n",
    "        result = np.zeros(wv.vector_size)\n",
    "    else:\n",
    "        result = np.sum(vecs, axis=0) / ( np.sum(weight) + 1e-9 )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed = [ sent_embed_mean(token) for token in X_tokens ]\n",
    "X_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1517b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed2 = [ sent_embed_tfidf(token) for token in X_tokens ]\n",
    "X_embed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a91294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 2개 객체 생성 \n",
    "svc = SVC(random_state= 42)\n",
    "logi = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, Y, model, test_size = 0.2):\n",
    "    # X는 독립 변수\n",
    "    # Y는 종속 변수\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size= test_size, random_state=42, stratify=Y\n",
    "    )\n",
    "    # 모델에 학습\n",
    "    model.fit(X_train, Y_train)\n",
    "    # 학습된 모델에 예측 값\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"정확도 : \", round(\n",
    "        accuracy_score(y_pred, Y_test), 4\n",
    "    ))\n",
    "    print('분류 레포드 : ')\n",
    "    print(classification_report(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed, Y, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed, Y, logi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999087e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed2, Y, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(X_embed2, Y, logi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321109d3",
   "metadata": {},
   "source": [
    "df에서 하위 10개 데이터를 이용하여 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef147ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델에 예측의 값을 반환하는 함수 \n",
    "# 세번째 매개변수(vec_type)를 생성 -> 기본값은 'mean'\n",
    "# 'tfidf' 입력이 들어온다면 벡터화 작업은 w2v + ifidf 융합한 벡터화 \n",
    "def predict_sentence_list(sentences, model, vec_type = 'mean'):\n",
    "    # sentences : 문장들의 리스트 \n",
    "    # 문장들을 토큰화 -> 임베딩 \n",
    "    X_test = []\n",
    "    for sent in sentences:\n",
    "        # token() 함수를 호출하여 토큰화 \n",
    "        tokens = tokenize(sent)\n",
    "        # 토큰화 된 문장을 sent_enbed_mean 함수에 입력하여 호출 ( 단위 백터의 평균 )\n",
    "        if vec_type == 'mean':\n",
    "            vec = sent_embed_mean(tokens)\n",
    "        elif vec_type == 'tfidf':\n",
    "            vec = sent_embed_tfidf(tokens)\n",
    "        X_test.append(vec)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    result = []\n",
    "    for sent, pred in zip(sentences, preds):\n",
    "        label = \"긍정\" if pred == 1 else \"부정\"\n",
    "        result.append([sent, label])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf37390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 -> 예측\n",
    "X_test = df['document'].tail(10).values\n",
    "run_model(X_embed, Y, svc)\n",
    "\n",
    "predict_sentence_list(X_test, svc, vec_type='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_model(X_embed2, Y, logi)\n",
    "predict_sentence_list(X_test, logi, vec_type='tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
