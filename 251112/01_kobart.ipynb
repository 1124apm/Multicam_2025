{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5607f2",
   "metadata": {},
   "source": [
    "#### BART\n",
    "- transformer 기반의 모델\n",
    "- Encoder, Decoder 모두 사용하는 모델\n",
    "    - Encoder: 입력되는 문장 데이터를 BERT 형식으로 이해 ( 순방향, 역방향 )\n",
    "    - Decoder: 출력되는 요약문을 GPT 형식으로 생성\n",
    "- 장문의 텍스트에서 요약 데이터를 생성하는 데 사용\n",
    "- tokenizer로 `Sentencepiece` 이용\n",
    "    - 인풋 tokenizer (문장 이해) 와 아웃풋 tokenizer (문장 생성) 를 따로 사용\n",
    "        - 인풋 tokenizer에서는 document의 시작과 끝을 나타내는 특수 토큰들을 사용\n",
    "        - 파이썬은 이식성, 확장성이 좋아, 아웃풋 tokenizer에서도 해당 특수 토큰들을 사용할 수도 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880a33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자형 데이터를 이용해 검증 지표를 만드는 데 사용되는 라이브러리\n",
    "# !pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5e9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate     # 문장 간의 검증 지표를 만들어주는 라이브러리\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764c98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 모델 선정 (kobart - 아직 완벽하지는 않은 모델)\n",
    "model_name = 'gogamza/kobart-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554376f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에서 사용할 원문 데이터, 요약 데이터\n",
    "train_docs = [\n",
    "    '정부는 중소기업 세제 혜택과 R&D 세액 공제를 확대한다고 밝혔다',\n",
    "    '해당 기업은 분기 실적에서 매출 성장을 기록했으며 신제품 출시를 예고했다'\n",
    "]\n",
    "train_sums = [\n",
    "    '정부가 중소기업 지원을 확대한다',\n",
    "    '기업이 실적 개선과 신제품 출시를 발표했다'\n",
    "]\n",
    "\n",
    "valid_docs = [\n",
    "    '교육부가 디지털 교과서 도입을 추진한다고 발표했다'\n",
    "]\n",
    "valid_sums = [\n",
    "    '교육부가 디지털 교과서 도입을 추진한다'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer 모델에서 list 형태의 데이터 사용 불가\n",
    "# 따라서 Dataset 형태의 데이터를 불러와, Dataset들로 이루어진 Dict 형태로 만들어준다. (value가 Dataset)\n",
    "raw_ds = DatasetDict(\n",
    "    {\n",
    "        # 학습 데이터\n",
    "        'train': Dataset.from_dict(\n",
    "            {\n",
    "                'document': train_docs,     # 원본 문장 (input)\n",
    "                'summary': train_sums       # 요약 문장 (output)\n",
    "            }\n",
    "        ),\n",
    "        # 검증 데이터\n",
    "        'validation': Dataset.from_dict(\n",
    "            {\n",
    "                'document': valid_docs,\n",
    "                'summary': valid_sums\n",
    "            }\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Dataset은 맨 앞 글자가 대문자이므로 class라는 것을 알 수 있고, class에는 속성(변수), 메서드(함수)가 존재한다.\n",
    "# 그 중에는 pandas로부터 DataFrame 데이터를 가져와 Dataset으로 만들어주는 from_pandas라는 메서드가 있다.\n",
    "# 그 외에도 Dataset.from_{파일형식} 은 해당 형식의 데이터를 Dataset으로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3858444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54f1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저, 모델을 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast= True)\n",
    "# use_fast(가속모드): BERT 모델은 RUST 언어 기반인데, 일반적으로 SKT의 KoBERT에선 use_fast가 만들어져있지 않아 이전 코드들에선 False로 뒀다.\n",
    "# 그러나 KoBART 모델에는 RUST 기반의 가속모드가 존재하므로, 사용하는 편이 효율적.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 입력/출력 문장의 최대 길이 설정\n",
    "max_input_len = 512\n",
    "max_target_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609bc167",
   "metadata": {},
   "source": [
    "`sklearn`</br>\n",
    "```python\n",
    "X_train, X_test, y_train, y_test로 분할 + 성능을 올리기 위한 option\n",
    "-> fit(X_train, y_train)\n",
    "-> pred = predict(X_test)\n",
    "-> f1_score(y_test, pred)\n",
    "```\n",
    "`transformer` (BERT 모델)\n",
    "```python\n",
    "tokenizer + 성능을 올리거나 부족한 token 길이를 채우기 위한(padding) option\n",
    "-> Trainer - train, validation\n",
    "-> predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bffeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리, 토큰화\n",
    "def tok_fn(batch):\n",
    "    # 입력 데이터의 토큰화 -> 인코딩\n",
    "    inputs = tokenizer(\n",
    "        # batch의 인자값: 묶음형 데이터셋\n",
    "        # raw_ds에서 인풋 데이터는 document의 데이터\n",
    "        batch['document'],\n",
    "        max_length = max_input_len,\n",
    "        padding = 'max_length',     # 고정 길이 패딩 사용\n",
    "        truncation = True           # 문장이 최대 길이(max_token의 개수)보다 긴 경우 자른다.\n",
    "    )\n",
    "    # 출력 데이터의 토큰화 -> 인코딩\n",
    "    # 아웃풋 전용 토크나이저 사용\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch['summary'],\n",
    "            max_length = max_target_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = True\n",
    "        )\n",
    "    # input에서 사용한 토크나이저 != labels에서 사용한 토크나이저\n",
    "\n",
    "    # padding 토큰을 -100으로 변경 (CrossEntropyloss에서 -100이라는 값을 무시)\n",
    "    # tokenizer의 result:\n",
    "    #   - input_ids: 단어 사전에 있는 인덱스의 값들로 인코딩된 데이터 (토큰화된 Input Text)\n",
    "    #   - attention_mask: 각 토큰이 실제 단어인지(1) 패딩 토큰(0)인지 나타냄\n",
    "    #   - token_type_ids: 문장의 위치\n",
    "\n",
    "    # 값을 바꾸기 용이하도록 Dataset에서 numpy array 형태로 바꿈\n",
    "    labels_ids = np.array(labels['input_ids'])  # 실제 문장에 대한 토큰값\n",
    "    # 행렬에서 pad_token_id(사용하는 tokenizer에서 <PAD> 토큰의 위치)와 같은 값을 가진 데이터를 -100으로 변경\n",
    "    labels_ids[ labels_ids == tokenizer.pad_token_id ] = -100\n",
    "    # labels_ids를 inputs에 있는 labels에 대입\n",
    "    inputs['labels'] = labels_ids.tolist()\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "811d6a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00,  4.53 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 224.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tok_fn 호출\n",
    "# raw_ds의 데이터를 tok_fn에 대입\n",
    "tokenized_ds = raw_ds.map(\n",
    "    tok_fn,\n",
    "    batched= True,\n",
    "    remove_columns= ['document', 'summary']     # 필요 없는 컬럼 제거\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4c57b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0495b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollator: padding 토큰의 동적인 처리 + 모델 입력 형태 자동 구현\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer= tokenizer,\n",
    "    model= model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b09e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.27kB [00:00, 1.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 검증 지표 선택\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c45814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # 디코더의 역할: vocab의 인덱스로 구성된 리스트를 다시 문자 형태로 변환\n",
    "    #   예: preds = [13, 17, 102, ...(사전 vocab=['A', 'B', ...]에서 해당 값의 위치)] 처럼 labels 데이터와 같은 형태로 만들어질 것.\n",
    "    # padding 토큰을 무시하기 위해 -100으로 구성했던 것을, 원래의 id 값으로 전환\n",
    "    labels = np.where(\n",
    "        labels != -100, labels, tokenizer.pad_token_id\n",
    "    )\n",
    "    # 텍스트 디코딩\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)  # 특수 토큰들은 디코드하지 않음\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 문장의 좌우에 공백이 존재하는 경우 다른 값으로 측정하기 때문에 좌우 공백 제거\n",
    "    pred_str = [ doc.strip() for doc in pred_str ]\n",
    "    label_str = [ doc.strip() for doc in label_str ]\n",
    "\n",
    "    # Rouge 계산\n",
    "    result = rouge.compute(\n",
    "        predictions= pred_str,\n",
    "        references= label_str,\n",
    "        use_stemmer= True       # 점수 계산 시 단어의 어간을 기준으로 비교할 것인가?\n",
    "    )\n",
    "\n",
    "    # Rouge를 보기 편한 형태로 변경\n",
    "    result = { k: round(v * 100, 2) for k, v in result.items() }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdfccf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 파라미터 값을 지정\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir= \"./kobart\",\n",
    "    eval_strategy= 'epoch',\n",
    "    save_strategy= 'epoch',\n",
    "    learning_rate= 5e-5,\n",
    "    num_train_epochs= 10,           # 요약이 잘 나오게 하기 위해 많이 돌림 (5번 돌려보니 이상하게 나왔음)\n",
    "    logging_steps= 10,\n",
    "\n",
    "    # generate 설정 변경 (성능적인 부분)\n",
    "    predict_with_generate= True,\n",
    "    generation_max_length= 70,      # 출력 토큰의 최대 길이\n",
    "    generation_num_beams= 4,        # 요약 데이터를 생성할 때 문장 후보 탐색 개수\n",
    "\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model= 'rougeL',    # rouge1(단어별 확인), rouge2, rougeL(일반적. 가장 큰 경우)\n",
    "    greater_is_better= True,\n",
    "    report_to= []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e23b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_8896\\1379372643.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.601643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.585189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.075991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.548694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.685046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.791951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.976944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.056381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.094729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.527300</td>\n",
       "      <td>4.143718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.527338981628418, metrics={'train_runtime': 174.208, 'train_samples_per_second': 0.115, 'train_steps_per_second': 0.057, 'total_flos': 6097364582400.0, 'train_loss': 2.527338981628418, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer 생성\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model= model,\n",
    "    args= args,\n",
    "    train_dataset= tokenized_ds['train'],\n",
    "    eval_dataset= tokenized_ds['validation'],\n",
    "    tokenizer= tokenizer,\n",
    "    data_collator= data_collator,\n",
    "    compute_metrics= compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd53ee",
   "metadata": {},
   "source": [
    "---\n",
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7e7869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하겠다고 밝혔다. 스타트업 새당으로 GPU 리소스를 확대 제공할 계획이다.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"과학기술정보통신부는 초거대 AI 연구 인프라 지원을 강화한다고 밝혔다. 스타트업 새당으로 GPU 리소스를 확대 제공할 계획이다.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors = 'pt',\n",
    "    truncation = True,\n",
    "    max_length = max_input_len\n",
    ")\n",
    "inputs.pop('token_type_ids', None)\n",
    "\n",
    "gen_ids = model.generate(\n",
    "    **inputs.to(model.device),\n",
    "    max_length = 20,\n",
    "    num_beams = 4,\n",
    "    do_sample = False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
