{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c7d481",
   "metadata": {},
   "source": [
    "# 통계기반 자연어 처리</br><span style='color:#808080'>(Natural Language Processing, NLP)\n",
    "\n",
    "## NLP의 접근 방식\n",
    "1. 규칙 기반<span style='color:#808080'> (Rule-based)</span>\n",
    "    - 사람이 언어의 문법, 구문, 의미적 패턴을 직접 정의하여 처리\n",
    "    - 유연성이 낮고, 예외가 많아질수록 규칙 관리가 어려워짐. 초기 NLP 모델에서 사용됨.\n",
    "2. 통계 기반<span style='color:#808080'> (Statistical)</span>\n",
    "    - 대규모 텍스트 데이터에서 **단어의 빈도, 공동 발생 확률** 등의 통계적 특징을 이용해 패턴을 학습하고 언어를 처리\n",
    "    - N-gram, TF-IDF, Hidden Markov Model (HMM) 등이 대표적. 딥러닝 이전에 주류를 이룸.\n",
    "3. 딥러닝 기반<span style='color:#808080'> (Deep Learning)</span>\n",
    "    - *신경망 모델** (RNN, CNN, Transformer 등)을 활용하여 데이터로부터 복잡한 특징과 패턴을 자동으로 학습\n",
    "    - 대규모 데이터가 필요하며, 현재 가장 높은 성능을 보이는 접근 방식. (단어 임베딩 사용)\n",
    "\n",
    "### N-gram 근사 <span style='color:#808080'>(N-gram Approximation)\n",
    "- 특정 단어의 출현 확률을 앞의 $N-1$개의 단어에만 의존한다고 가정하고 근사하는 언어 모델\n",
    "- Unigram\n",
    "    - $N$ = 1\n",
    "    - $P(\\text{단어}_i)$ - 독립 단어의 확률만 고려 (문맥 무시)\n",
    "- Bigram\n",
    "    - $N$ = 2\n",
    "    - $P(\\text{단어}_i \\mid \\text{단어}_{i-1})$ - 바로 앞의 단어 하나만 고려\n",
    "- Trigram\n",
    "    - $N$ = 3\n",
    "    - $P(\\text{단어}_i \\mid \\text{단어}_{i-1}, \\text{단어}_{i-2})$ - 앞의 2개의 단어를 고려\n",
    "\n",
    "### 로그 확률 <span style='color:#808080'>(Log Probability)\n",
    "-문제점: 조건부 확률은 매우 작은 값(0.0001, 0.00001)이며, 이를 곱하게 되면 <span style='color:#808080'>(예: $P(문장) = P(단어_1) \\times P(단어_2 \\mid \\text{단어}_1) \\times \\dots$)</span> 결과가 0에 가까워져 컴퓨터의 계산(부동소수점 정밀도)이 불안정해지는 언더플로우(Underflow) 문제가 발생.\n",
    "단어의 조건부 확률은 매우 작은 값 (0.0001, 0.00001)\n",
    "- 조건부의 확률들 끼리 곱하게 되면 -> 0에 가까운 값 -> 계산이 불안정\n",
    "- 이러한 문제를 해결하기 위해 log 값을 사용\n",
    "\n",
    "### 혼란도 <span style='color:#808080'>(Perplexity, PP)\n",
    "- 문장을 얼마나 잘 예측하는지 나타내는 언어 모델의 성능 지표.</br>문장에 대한 역확률(Inverse Probability)의 기하 평균과 관련. 즉, **\"언어 모델이 문장을 만드는 데 얼마나 많은 '선택지'를 필요로 하는가?\"**\n",
    "- 값이 높다면 -> 문장 이해도가 내려간다.\n",
    "</br><span style='color:#808080'>모델이 문장을 헷갈려 하거나 다음 단어를 예측하기 어렵다는 뜻 (성능 낮음)\n",
    "- 값이 낮다면 -> 문장 이해도가 올라간다.\n",
    "</br><span style='color:#808080'>모델이 문장 구조를 잘 이해하고 다음 단어를 예측하기 쉽다는 뜻 (성능 높음)\n",
    "\n",
    "### BoW <span style='color:#808080'>(Bag of Words, 단어 가방 모델)\n",
    "- 문서의 단어 순서는 무시\n",
    "- 각 문서에서 단어가 몇번 등장했는가?(빈도수)\n",
    "- 가장 기본적인 백터화 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaa2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터셋 생성 \n",
    "docs = [\n",
    "    \"영화가 정말 재미있었다\", \n",
    "    \"영화가 너무 지루하다\", \n",
    "    \"배우의 연기가 너무 좋았다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c00f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['영화가', '정말', '재미있었다'], ['영화가', '너무', '지루하다'], ['배우의', '연기가', '너무', '좋았다.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [doc.split() for doc in docs]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2293aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens에서 각각의 단어들을 하나의 리스트로 생성하고 중복은 제거 \n",
    "# tokens의 2차원 리스트를 1차원으로 변환 \n",
    "vocab1 = []\n",
    "for token in tokens:\n",
    "    # token -> tokens의 각 원소들 (1차원 리스트)\n",
    "    for word in token:\n",
    "        # word : token이라는 1차원 리스트의 각각의 원소들 \n",
    "        vocab1.append(word)\n",
    "vocab1 = list(set(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edeca8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['연기가', '배우의', '지루하다', '재미있었다', '너무', '영화가', '좋았다.', '정말']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbc26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(sum(tokens, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba59f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bow 행렬을 하나 생성 \n",
    "# tokens의 문장에 vocab의 단어가 몇개 포함되어있는가?\n",
    "bow_list = []\n",
    "vocab.sort()\n",
    "for doc in tokens:\n",
    "    # row = [ doc.count(word) for word in vocab ]\n",
    "    row = []\n",
    "    for word in vocab:\n",
    "        # count() 함수는 list에서 인자의 값과 같은 데이터가 몇개 있는가?\n",
    "        row.append(doc.count(word))\n",
    "    bow_list.append(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
